

* **Date : 2022-06-20**
* **Last Devised : 2022-06-25**


# Introduction to Reinforcement Learning

<br>
<p align="center" style="color:gray">
    <img src="https://i0.wp.com/bdtechtalks.com/wp-content/uploads/2021/06/Reinforcement-learning.jpg">
</p>
<center><b>General Principle of Reinforcement Learning</b></center>
<br>

<!-- TOC -->

- [Introduction to Reinforcement Learning](#introduction-to-reinforcement-learning)
    - [Origins in Animal Learning](#origins-in-animal-learning)
        - [Turing’s Unorganised Machines](#turings-unorganised-machines)
    - [Origins in Optimal Control](#origins-in-optimal-control)
        - [What is the difference between reinforcement learning and optimal control?](#what-is-the-difference-between-reinforcement-learning-and-optimal-control)
    - [Learning Automata](#learning-automata)
    - [Hedonistic Neurons](#hedonistic-neurons)
        - [Overlap of Neurobiology and Reinforcement Learning](#overlap-of-neurobiology-and-reinforcement-learning)
    - [Temporal Difference](#temporal-difference)
        - [TD Gammon](#td-gammon)
    - [Q-Learning](#q-learning)
        - [Deep Reinforcement Learning and Deep Q-learning](#deep-reinforcement-learning-and-deep-q-learning)
        - [Google DeepMind and Video Games](#google-deepmind-and-video-games)
        - [AlphaGo](#alphago)
    - [Modern Developments](#modern-developments)
    - [Concluding Remarks](#concluding-remarks)
- [An introduction to Q-Learning : Reinforcement Learning](#an-introduction-to-q-learning--reinforcement-learning)
    - [The brass tacks: What is reinforcement learning?](#the-brass-tacks-what-is-reinforcement-learning)
    - [Recipes for reinforcement learning](#recipes-for-reinforcement-learning)
        - [Defining the problem statement](#defining-the-problem-statement)
        - [The states](#the-states)
        - [The actions](#the-actions)
        - [The rewards](#the-rewards)
        - [The Bellman Equation](#the-bellman-equation)
        - [Modeling Stochasticity : Markov Decision Processes](#modeling-stochasticity--markov-decision-processes)
    - [Transitioning to Q-Learning](#transitioning-to-q-learning)
    - [The Last Piece of the Puzzle : Temporal Difference](#the-last-piece-of-the-puzzle--temporal-difference)
    - [Implementing Q-Learning in Python with Numpy](#implementing-q-learning-in-python-with-numpy)
    - [Conclusion and further steps](#conclusion-and-further-steps)
    - [Reference](#reference)

<!-- /TOC -->

강화학습(Reinforcement Learning, RL)은 흥미롭고 빠르게 발전하는 [기계학습의 한 영역](https://researchdatapod.com/online-courses/top-online-courses-for-machine-learning/)이다. 

인터넷에서 머신러닝을 검색하면 가장 흔히 보이는 자료가 기계학습에는 지도학습(Supervised Learning), 비지도학습(Unsupervised Learning), 그리고 강화학습(Reinforcement Learning)의 3대장이 있다고 하는 다음과 같은 그림일 것이다. 하지만 이 설명은 별로 정확한 것 같지 않다.

<br>
<p align="center" style="color:gray">
    <img src="https://kr.mathworks.com/discovery/reinforcement-learning/_jcr_content/mainParsys3/discoverysubsection/mainParsys/image.adapt.full.medium.png/1647932661836.png">
</p>
<center><b>Three Categories of Machine Learning</b></center>
<br>

지도학습이나 비지도학습은 어떤 모델이 주어진 데이터의 패턴을 학습하는 것과 관련이 있지만, 강화학습은 에이전트(Agent)와 환경(Environment)의 상호작용이라는 완전히 다른 학습 원리로 작동하기 때문이다. 

실제로 강화학습의 역사를 살펴보면, 실험심리학의 동물학습과 최적제어이론 분야에서 비롯되었으며 신경과학분야의 학문적 기여도 포함되어 있다. 따라서 강화학습은 지도학습/비지도학습과는 완전히 다른 분야로 생각하는 것이 더 자연스럽다.

강화학습은 보통 올바른 결정을 통해 받는 보상(Reward)의 발생을 최대화하여 최적화된 일련의 행동(Action)으로 문제를 해결하는 것에 중점을 둔다. 개인적으로 현대의 여러 인공지능 분야 가운데 가장 인위적인 지능의 구현에 가깝다고 느끼는 분야이다. 

이번 포스팅에서는 강화학습의 기원에서 현대에 이르기까지의 역사를 간단히 정리하고, 기본적인 강화학습 예제를 구현하려고 한다. 하지만 내가 이전까지 작성했던 다른 머신러닝 포스팅과는 달리, 여기서 강화학습의 원리나 관련연구에 대해 자세히 다루지는 않을 것이다. 그 이유는 내가 강화학습을 공부한 지 얼마되지 않은 Newbie이기 때문이다. 따라서 이제부터 강화학습을 공부하며, 알게 되는 내용을 덧붙여서 글을 조금씩 수정해나가려고 한다.

<br>
<p align="center" style="color:gray">
    <img src="https://i0.wp.com/researchdatapod.com/wp-content/uploads/2021/09/The-History-of-Reinforcement-Learning.png?zoom=2&resize=529%2C1319&ssl=1">
</p>
<center><b>Infographic for History of Reinforcement Learning</b></center>
<br>



## Origins in Animal Learning

강화학습의 기원은 동물학습(Animal Learning)과 최적제어(Optimal Control)의 두 갈래로 나뉜다.

동물학습을 시작으로 Edward Thorndike는 1911년, ["Law of Effect"](https://link.springer.com/referenceworkentry/10.1007%2F978-0-387-79061-9_1624)로 시행착오 학습(Trial-and-Error Learning)의 본질을 설명했다. 

즉, 동물은 만족을 강화하고 불편을 초래하는 행동을 단념하면서 행동의 반복을 추구할 것이라고 말했다. 또한 쾌락이나 고통의 정도가 클수록 행동을 추구하거나 억제하는 경향도 클 것이라고 했다.

효과의 법칙은 긍정적인 자극으로부터 행동을 강화하는 효과를 설명하며, 행동의 미래 설명의 기본정의원칙(Basic Defining Principle)으로 널리 간주되었다.

효과의 법칙은 선택(Selectional) 학습과 연관(Associative) 학습을 결합하는 설명이었다. 여기서 선택학습은 대안을 시도하고 결과에 따라 선택하는 것을 포함하고, 연관학습은 특정 상황과 관련된 선택들 중에서 특정한 선택 사항을 찾는 것을 의미한다.

"강화(Reinforcement)"라는 용어는 1927년, [Pavlov](https://www.simplypsychology.org/pavlov.html)에 의해 동물학습의 맥락에서 공식적으로 사용되었다. 그는 강화를 다른 자극이나 반응과의 시간의존적 관계에서 "자극(Reinforcer, 강화인자)"를 받는 동물로 인한 행동패턴의 강화라고 설명했다.

<br>
<p align="center" style="color:gray">
    <img src="https://researchdatapod.com/wp-content/uploads/2021/11/thorndike-cat-box-980x600.jpeg">
</p>
<center><b>Thorndike’s Cat Box</b></center>
<br>



### Turing’s Unorganised Machines

1948년 Alan Turing은 ["Intelligent Machinery"](https://oxford.universitypressscholarship.com/view/10.1093/oso/9780198250791.001.0001/isbn-9780198250791-book-part-16)라는 보고서에서 지능적 행동을 할 수 있는 기계를 구축할 수 있는 전망에 대한 조사를 발표했다. 

Turing은 계산을 수행하기 위해 무작위로 연결된 뉴런과 같은 노드(Neuron-like Node)의 네트워크를 사용하는 것을 제안한 최초의 사람이었을 것이다. 

그는 어린이에게 가르치는 것처럼 훈련될 수 있는, 대규모 뉴런으로 구성된 뇌와 같은(Brain-like) 네트워크의 구성을 제안했다. Turing은 그의 네트워크를 "조직화되지 않은 기계(Unorganized Machine)"라고 불렀다.

Turing은 조직화되지 않은 기계의 3가지 유형을 설명했다. A-type 및 B-type 비조직화 기계는 무작위로 연결된 2상태 뉴런(Two-state Neuron)으로 구성된다. 뉴런과 유사하지 않은 P형 조직화되지 않은 기계는 "쾌락이나 보상을 위한 입력과 고통이나 처벌을 위한 입력의 두 가지 간섭입력"만 있었다.

Turing은 어린이 학습(Children Learning)과 유사한 훈련 절차를 시도하고 발견하기 위해 P형 기계를 연구했다. 그는 "적절한 추론(Appropriate Inference), 모방 교육(Mimicking Education)"을 적용하면, B형 기계가 "충분한 시간과 장치가 제공된다면 필요한 모든 작업을 수행하도록 훈련"될 수 있다고 말했다.

시행착오 학습(Trial-and-error Learning)은 많은 전기역학적인 기계의 생산으로 이어졌다. 

1933년, [Thomas Ross](https://history-computer.com/thomas-ross-complete-biography/)는 간단한 미로를 통해 길을 찾고, 스위치 구성을 통해 경로를 기억할 수 있는 기계를 만들었다. 

1952년, Claude Shannon은 시행착오를 사용하여 미로를 탐색하는 테세우스라는 이름의 미로실행 쥐(Maze-Running Mouse)를 시연했다. 미로는 바닥 아래에 있는 자석과 릴레이(Relay)를 사용하여 연속적인 방향을 기억할 수 있었다. 

1954년, Marvin Minsky는 계산 강화학습 방법(Computational Reinforcement Learning Method)에 대해 논의하고, SNARC(Stochastic Neural-Analog Reinforcement Calculators)라는 구성요소로 만들어지는 아날로그 기계의 구조를 설명했다. SNARC는 뇌의 수정가능한 시냅스 연결(Modifiable Synaptic Connections)과 유사하도록 고안되었다. 

1961년, Minsky는 기여했을 수 있는 결정 사이에서 성공에 대한 신용을 분배하는 방법을 설명하는 신용할당문제(Credit Assignment Problem)를 해결했다.

계산 시행착오 과정에 대한 연구는 결국 오류정보를 사용하여 뉴런 연결 가중치(Neuron Connection Weight)를 업데이트하는 오늘날의 지도학습에 흡수되었다. 그러나 그 전에 먼저 패턴인식(Pattern Recognition)으로 일반화되었다. RL에 대한 연구는 1960년대와 1970년대에 걸쳐 거의 사라졌다.

그러나 1963년, 상대적으로 알려지지 않았지만 John Andreae는 환경과의 상호작용을 통해 학습하는 [STELLA 시스템](http://cyberneticzoo.com/wp-content/uploads/2010/01/STL556.pdf) 및 [Machines with an "internal monologue" (내부독백기계)](https://www.sciencedirect.com/science/article/abs/pii/S0020737369800088) 그리고 나중에 [교사에게 배울 수 있는 기계](https://www.researchgate.net/publication/273636297_Thinking_with_the_Teachable_Machine)를 포함하여 선구적인 연구를 개발했다.

<br>
<p align="center" style="color:gray">
    <img src="https://researchdatapod.com/wp-content/uploads/2021/11/snarc-980x392.webp">
</p>
<center><b>Photo of Minsky’s SNARC</b></center>
<br>



## Origins in Optimal Control

최적제어(Optimal Control) 연구는 1962년 Pontryagin과 Neustadt가 보여준 바와 같이, 연속시간 제어문제(Continuous Time Control Problem)에서 제어정책(Control Policy)을 도출하는 최적화 방법을 정의하기 위한 공식 프레임워크로 1950년대에 시작되었다. 

Richard Bellman은 제어문제를 풀기 위해, 수학 최적화 및 컴퓨터 프로그래밍 방법으로 동적 프로그래밍(Dynamic Programming)을 개발했다. 이 프로세스는 동적 시스템의 상태를 사용하여 기능 방정식(Functional Equation)을 정의하고, 최적값 함수(Optimal Value Function)라고 하는 것을 반환한다. 최적 함수는 일반적으로 벨만 방정식(Bellman Equation)에서 얻어졌다.

Bellman은 최적제어문제의 이산확률적 버전(Discrete Stochastic Version of the Optimal Control Problem)으로 정의하는 Markovian Decision Process(MDP)를 도입했다. 

Ronald Howard는 [1960년에 MDP를 위한 정책반복 방법(Policy Iteration Method for MDP)](https://www.jstor.org/stable/2629352)을 고안했다. 이 모든 것은 현대 강화학습의 이론과 알고리즘을 뒷받침하는 필수요소이다.



### What is the difference between reinforcement learning and optimal control?

발생하는 일반적인 질문은 최적제어와 강화학습의 차이점이 무엇인지이다. 
현대의 강화학습 관련 연구들은 이전 세대에 이루어진 최적제어 분야의 모든 연구를 높이 평가한다.

강화학습 문제는 최적제어문제, 특히 MDP로 공식화되는 것과 같은 확률론적 문제와 밀접하게 관련되어 있다. 동적 프로그래밍과 같은 최적제어의 솔루션 방법도 강화학습 방법으로 간주되며, 연속적인 근사(Successive Approximation)를 통해 점차 정답에 도달한다. 

강화학습은 결국, 최적제어에서 비전통적인 제어문제(Non-traditional Control Problem)로 아이디어를 일반화하거나 확장하는 것으로 생각할 수 있다.



## Learning Automata

1960년대 초에 학습자동장치(Learning Automata)에 대한 연구가 시작되었으며, 그 역사는 소련의 Michael Lvovitch Tsetlin까지 거슬러 올라갈 수 있다. 

학습자동장치는 환경과의 반복적인 상호작용을 통해 최적의 행동을 학습하는 무작위 환경에 위치한 적응형 의사결정장치(Adaptive Decision-making Unit situated in a Random Environment)이다. 단계는 환경의 응답을 기반으로 하는 특정 확률분포에 따라 선택된다. 

학습 오토마타는 RL에서 정책 반복자(Policy Iterators)로 간주된다. Tsetlin은 Tsetlin Automaton을 고안했는데, 이는 인공뉴런보다 훨씬 더 근본적이고 다재다능한 학습 메커니즘으로 간주된다. Tsetlin Automaton은 잘 알려진 Multi-Armed Bandit 문제에 대한 선구적인 해답 중 하나이며, 패턴분류에 계속 사용되었다. 또한 Decentralized Control과 Equi-partitioning 및 Faulty Dichotomous Search을 포함하여, 더욱 진보된 학습 오토마타 설계의 핵심을 형성했다. 



## Hedonistic Neurons

1970년대 후반과 1980년대 초반에, [Harry Klopf](https://www.amazon.co.uk/Hedonistic-Neuron-Theory-Learning-Intelligence/dp/089116202X)는 자연지능을 설명하고 기계지능의 기초를 제공하기 위한 평형추구과정(Equilibrium-seeking Process)에 초점을 맞추는데 만족하지 못했다. 여기에는 항상성(Homeostasis) 및 일반적으로 지도학습과 관련된 오류수정학습(Error-correction Learning)이 포함되었다.

그는 하나의 수량(Quantity)을 최대화하려는 시스템은 균형추구 시스템(Equilibrium-seeking System)과 질적으로 다르다고 주장했다. 

또한, 그는 시스템을 극대화하는 것이 자연지능의 중요한 측면을 이해하고 인공지능을 구축하는데 필수적이라고 주장했다. Klopf는 뉴런이 개별적으로 쾌락주의적(Individually Hedonistic)이라는 가설을 세웠는데, Neuron-local of Pain은 최소화하면서 Neuron-local Analogue of Pleasure를 최대화하기 위해 작동한다는 점을 근거로 들었다.

<br>
<p align="center" style="color:gray">
    <img src="https://i0.wp.com/researchdatapod.com/wp-content/uploads/2021/11/Neuron.jpg?resize=796%2C427&ssl=1">
</p>
<center><b>Annotated Diagram of a Neuron</b></center>
<br>

쾌락주의적 뉴런에 대한 Klopf의 아이디어는 뉴런이 효과법칙(Law of Effect)의 Neuron-local Version을 구현한다는 것이었다. 그는 뉴런의 시냅스 가중치(Synaptic Weights of Neurons)가 경험에 따라 변한다는 가설을 세웠다. 

뉴런이 활동전위(Action Potential)를 촉발하면, 활동전위에 기여하는 모든 시냅스가 효능을 변경한다. 활동 전위가 보상되면 모든 적격(Eligible) 시냅스의 효율성이 증가하거나, 처벌을 받으면 감소한다). 

따라서 시냅스는 뉴런의 발화 패턴을 변경하여 뉴런이 보상을 받을 확률을 높이고, 환경에 의해 불이익을 받을 가능성(Likelihood)을 줄이도록 변경된다.

이 가설은 본질적으로 평형추구과정(Equilibrium-Seeking Process)인 지도학습과, 학습자의 결정이 경험에 따라 진화하는 효과적인 평가주도시스템(Evaluation-Driven System)인 강화학습 사이에 상당한 차이를 만들어내는 것이었다. 

오류수정과 RL 모두 최적화 프로세스이지만, 오류수정은 더 제한적이며, RL은 행동 최적화를 통해 보상을 최대화하여 일반화되고 행동을 수정한다.



### Overlap of Neurobiology and Reinforcement Learning

[신경생물학](https://www.sciencedirect.com/science/article/pii/S0149763418305530)은 피질-소뇌-기저핵 시스템(Cortex-Cerebellum-Basal Ganglia System) 내에서 비지도, 지도 및 강화학습과 같은 다양한 형태의 학습을 탐구했다. 이러한 학습과정을 분리하고 그 구현을 별개의 뇌 영역에 할당하는 것은 신경과학 연구의 근본적인 도전과제였다. 

1995년 Houk와 Wise, 1997년 Schultz가 제안한 여러 데이터에 따르면 신경조절 도파민(Neuromodulation Dopamine)은 강화학습 프로세스에 영향을 줄 수 있는 보상예측 오류(Reward Prediction Error)를 전달하는 위상신호를 기저핵 표적구조(Basal Ganglia Target Structure)에 제공한다. 

따라서 기저핵의 기능을 도파민 피드백에 의해 안내되는 가능한 행동의 공간을 통한 추상적인 탐색으로서 특성화하는 것이 가능하다.

최근 연구에서는 3가지 뇌 영역이 유연한 운동 행동을 학습할 수 있도록 하는 [Super-learning Process](https://www.sciencedirect.com/science/article/pii/S0149763418305530)로 서로 다른 학습 메커니즘을 결합하는 고도로 통합된 시스템을 형성하는 방법을 탐구했다. Super Learning은 고립된 것과는 대조적으로 시너지 효과로 작용하는 다른 학습 메커니즘을 의미한다.



## Temporal Difference

Temporal Difference(TD) Learning은 수학적 미분에서 영감을 얻었으며, 지연된 보상(Delayed Reward)으로부터 정확한 보상예측을 구축하는 것을 목표로 한다. 

TD는 즉각적인 보상과, 다음 단계에서의 보상 예측의 조합을 예측하려고 시도한다. 다음 시간단계(Time Step)가 도착하면, 최신예측이 새로운 정보로 예상했던 것과 비교된다. 

차이가 있는 경우 알고리즘은 오류를 계산하는데, 이는 "시간적 차이"로 이전 예측을 최신 예측으로 조정한다. 이 알고리즘은 모든 단계에서 이전 예측과 새 예측을 더 가깝게 만들어, 전체 예측 체인이 점진적으로 더 정확해지도록 하는 것을 목표로 한다.

TD 학습은 [1984년 박사학위 논문](https://dl.acm.org/doi/10.5555/911176)에서 TD 학습을 다루었으며, [1988년 논문](https://link.springer.com/article/10.1007/BF00115009)에서 Temporal Difference라는 용어가 처음 사용된 Sutton과 가장 밀접하게 관련되어 있다.

시간차 방법의 기원은 동물학습 이론, 특히 2차 강화물의 개념(Concept of Secondary Reinforcer)에 의해 강한 영감을 받았다. 2차 강화물은 1차 강화물과 짝을 이루는 자극이다. (예 : 음식의 존재) 2차 강화물은 1차 강화물과 유사한 속성을 취한다. 

시간차 방법은 [1972년과 1975년 Klopf](https://www.semanticscholar.org/paper/Brain-Function-and-Adaptive-Systems%3A-A-Heterostatic-Klopf/0c1accd2ef7218534a1726a8de7d6e7c14271a75)가 보다 광범위한 절차의 개별 하위 구성요소(Individual Sub-Components)로 분해된 대규모 시스템에서 강화학습을 탐구할 때의 시행착오 방법과 얽혀있다. 이 때 각각은 서로를 강화할 수 있었다.

동물학습이론을 시간적 신용할당문제(Temporal Credit Assignment Problem)를 포함하여, 시간적으로 연속적인 예측의 변화(Changes in Temporally Successive Prediction)에 의해 주도되는 학습 방법과 통합함으로써, 강화학습 연구의 폭발적 증가를 이끌어내었다. 

특히 1983년, Barto et al.의 극균형 문제(Pole-Balancing Problem)에 적용된 "Actor-Critic Architecture"에서 Actor-Critic Method는 가치함수(Value Function)와 무관한 정책(Policy)을 명시적으로 나타내기 위해 별도의 메모리 구조를 갖는 TD 방식이다. 

여기서 행위를 선택하는데 사용되는 정책구조(Policy Structure)를 행위자(Actor)라 하고, 행위자가 한 행위를 비판하는 추정가치함수(Estimated Value Function)를 비평가(Critic)라고 한다. Critique는 TD error의 형태를 취하는데, 그것은 비평가(Critic)의 유일한 출력이며, 배우(Actor)와 비평가(Critic) 모두에게 모든 학습을 유도했다.

1984년과 1986년, Actor-Critic Architecture는 확장되어 Backpropagation Neural Network Techniques를 통합하게 되었다.


### TD Gammon

1992년, [Gerry Tesauro](https://researcher.watson.ibm.com/researcher/view.php?person=us-gtesauro)는 주사위 놀이 지식(Backgammon Knowledge)이 거의 필요하지 않지만 그랜드마스터 수준에서 게임을 하는 방법을 배운 프로그램을 개발했다. 

학습 알고리즘은 TD 오류를 역전파하여 훈련된 다층 신경망을 사용하여, TD-Lambda Algorithm과 비선형 함수근사(Non-linear Function Approximation)를 결합했다.

TD-Gammon의 성공과 추가 분석을 기반으로, 최고의 인간 플레이어는 이제 알고리즘에 의해 학습된 파격적인 오프닝 포지션을 플레이하게 되었다.

<br>
<p align="center" style="color:gray">
    <img src="https://i0.wp.com/researchdatapod.com/wp-content/uploads/2021/10/gtesauro_ai_350.jpeg?w=350&ssl=1">
</p>
<center><b>Gerald Tesauro with TD-Gammon</b></center>
<br>



## Q-Learning

Chris Watkins는 1989년 자신의 박사학위 논문 "Learning from Delayed Rewards"에서 Q-learning을 소개했는데, 이 논문에서는 Markovian Decision Process의 제어를 점진적으로 최적화하는 강화학습 모델을 소개했다. 그리고 Markovian Decision Process의 전환 확률 또는 보상기대값을 모델링하는것 없이 직접 최적의 제어를 학습하는 방법으로 Q-Learning을 제안했다. 또한 Watkins와 Peter Dayan은 1992년에 수렴 증명을 제시했다. 

Q-Value Function은 정책을 따르는 에이전트의 상태가 주어졌을 때, 특정 행동이 얼마나 좋은지를 보여준다. Q-Learning은 Q-Function이 결국 $Q^*$로 수렴될 때까지 Bellman Equation을 사용하여 각 상태-동작 쌍(State-Action Pair)에 대한 Q-Value를 반복적으로 업데이트(Iteratively Updating)하는 프로세스이다. 

Q-Learning은 모델이 없는(Model-Free) 강화학습 알고리즘이며 적응(Adaptation)없이 확률적 전환(Stochastic Transition) 및 보상을 처리할 수 있었다. 이러한 혁신은 강화학습에 대한 상당한 후속 연구를 자극했다.



### Deep Reinforcement Learning and Deep Q-learning

<br>
<p align="center" style="color:gray">
    <img src="https://i0.wp.com/researchdatapod.com/wp-content/uploads/2021/10/goplayer.jpeg?resize=772%2C436&ssl=1">
</p>
<center><b>At the time, World Number 1 Go player Ke Jie during his second match against AlphaGo in Wuzhen, 2017.</b></center>
<br>

1980년대 중반부터 신경망에 대한 관심이 높아지면서, 신경망이 정책(Policy)이나 가치함수를 나타내는 Deep Reinforcement Learning에 대한 관심도 높아졌다. 

TD-Gammon은 신경망을 사용한 강화학습의 첫번째 성공적인 응용 프로그램이었다.

2012년, 컴퓨터 비전을 위한 그래픽 처리장치에서의 컨볼루션 신경망을 빠르게 구현함으로써 딥러닝 혁명이 촉발되었다. 이로 인해 다양한 영역에서 함수근사로 심층신경망을 사용하는 것에 대한 관심이 높아졌다. 

신경망을 적용하면 에이전트가 학습함에 따라 Q-Value Table을 직접 업데이트하는 Value Iteration Algorithm을 대체하는데 특히 유용했다.

Value Iteration은 상태 공간이 작은 작업에 적합하지만, 더 복잡한 환경이 있는 경우 새로운 상태를 탐색하고 Q-값을 수정하는데 필요한 계산 리소스 및 시간의 수는 극도로 제한되거나 실행불가능한 수준이었다. 

그러나 신경망을 사용하면 Value Iteration을 통해 직접 Q-값을 계산하는 대신, 함수 근사의 방식으로 최적의 Q-Function을 추정할 수 있었다. 이 경우 신경망은 환경으로부터 상태를 입력으로 받고, 에이전트가 해당 상태에서 선택할 수 있는 각 행동에 대해 추정된 Q-value을 출력한다. 그리고 해당 값을 $Q^*$ 목표값과 비교하여 손실을 계산하고, 역전파 및 확률적 경사하강법을 사용하여 가중치를 업데이트하는 방식으로 오류를 최소화하고 에이전트의 최적행동에 수렴하는 Q-값을 얻을 수 있었다.



### Google DeepMind and Video Games

2013년경, DeepMind는 컨볼루션 신경망 아키텍처와 Q-러닝을 결합한 Deep Q-Learning(DQN)을 개발했다. 

Deep Q-Learning은 상태를 저장하고 재생하는 Experience Replay를 용이하게 하고, 네트워크가 교육 왜곡(Skewing Training)을 방지하고 구현속도를 높이기 위해 작은 배치크기로 학습할 수 있도록 한다. 

연구팀은 Space Invaders 및 Breakout과 같은 비디오 게임에서 시스템을 테스트했다. 그 결과 코드를 변경하지 않고 네트워크는 게임을 하는 방법을 배웠고, 몇 번의 반복 후 인간의 성능을 능가할 수 있었다. DeepMind는 Seaquest 및 Q*Bert와 같은 다른 게임에서 인간의 능력을 능가하는 시스템에 대한 추가 연구를 발표했다.

![DeepMind Algorithhm learning the game Breakout](https://youtu.be/Q70ulPJW3Gk)



### AlphaGo

2014년, [DeepMind](https://www.deepmind.com/blog/deep-reinforcement-learning)는 바둑을 할 수 있는 컴퓨터 프로그램에 대한 연구를 발표했다. 

2015년 10월, [AlphaGo](https://arxiv.org/pdf/1812.06855.pdf)라는 컴퓨터 바둑 프로그램이 유럽 바둑 챔피언 Fan Hui를 이겼다. 이번 대회는 인공지능이 바둑 전문플레이어를 패비시킨 최초의 대회였다. 

2016년 3월, 알파고는 5경기에서 4-1로 세계랭킹 1위 바둑기사인 이세돌을 꺾었다. 

2017년 Future of Go Summit에서, AlphaGo는 2년 동안 세계 랭킹 1위였던 Ke Jie와 3연전에서 승리했다. 

그해 말, AlphaGo의 개선된 버전인 AlphaGo Zero는 AlphaGo를 100대 0으로 물리쳤다. 게임 방법을 배우는데 몇 달이 걸렸던 AlphaGo에 비해, 새로운 AlphaGo Zero는 더 적은 처리능력으로 3일만에 이전 버전의 성능을 압도했다.

2017년 Google의 AlphaZero 연구에서, 시스템은 5000개의 1세대 텐서처리장치(Tensor Processing Units, TPU)와 64개의 2세대 TPU를 사용하여 훈련 4시간만에 초인적인(Superhuman Level) 수준의 체스를 할 수 있었다. 

개인적으로 바둑을 어릴적부터 취미로 조금씩 두었기 때문에, 알파고 바둑을 관전하던 2016년(내가 학부 1학년생이었을 때) 알파고의 모든 대국을 여러번 봤었다. 

1국에서는 이세돌이 처음부터 변칙적으로 바둑을 두며 싸움을 이어나갔는데, 알파고의 대응과 수읽기는 인간바둑기사의 실력처럼 매우 강했고, 결국 최초로 바둑기사가 기계에게 패배하는 것을 보고 놀라움을 느꼈다. 

알파고와 이세돌의 제 2국에서는 더더욱 놀라운 수가 연발로 터져나왔던 것을 기억한다. 초반 포석에서 인간이라면 99% 반드시 두는 자리를 포기하고 다른 곳으로 손을 빼는 것도 놀라웠지만, 가장 압권이었던 것은 바둑 역사상 거의 존재하지 않았을 "5선 어깨짚기"였다. 그리고 이외에도 바둑을 보통 사람이라면, 그리고 바둑을 점점 더 잘두게 될수록 잘 두지 않게 되는 여러가지 패턴을 보여주었다.

내 기억으로 그 당시 가장 잘 만들어졌다고 알려진 일본의 바둑프로그램 Gen조차도 프로 바둑기사를 상대로 2점 접바둑을 두어야 대등한 기량을 펼칠 수 있었다. 일반적으로 2점 접바둑이라고 하면 인간의 기준에서는 그 실력차를 좁히는데 10년의 시간이 걸릴수도 있을 정도로 큰 차이이다. 

따라서 나를 비롯해서 바둑 커뮤니티의 대부분의 사람들은 당연히 이세돌이 압승할 것이라 생각했다. 나는 이때 인공지능에 전혀 관심이 없던 시절이지만, 그런 내 눈에도 확실히 알파고의 능력은 굉장했다. 그리고 이제서야 그 기반이 되는 강화학습을 공부할 시간이 생겼다. 



## Modern Developments

딥마인드가 개발한 AlphaFold는 전산생물학이 추구하는 가장 중요한 목표 중 하나인 아미노산 접힘(Amino Acid Folding)에 인공지능을 적용한 것으로, 새로운 효소설계와 같은 생명공학 및 약물설계와 같은 의약 응용분야에서 연구자들의 필수적인 도구가 되었다. 얼마 전에 화학과의 계산화학 전공교수님께서 학생들에게 AlphaFold 사용법을 가르치는 영상을 보았는데 매우 인상적이었다.

이외에도 강화학습은 제한된 환경 안에서 문제를 해결하는 상황에서 높은 성능을 보여주었다. 여기에는 로봇공학, 구조화된 의료이미지 처리, 자율주행 자동차 등이 포함된다.

<br>
<p align="center" style="color:gray">
    <img src="https://researchdatapod.com/wp-content/uploads/2021/11/deep-mind-alpha-fold-980x352.png">
</p>
<center><b>Amino acid folding</b></center>
<br>

한편, 강화학습을 더 효율적으로 만들기 위한 연구도 진행되었다. Google Brain은 에이전트 풀 전체에서 선택적 정보공유(Selective Information Sharing)를 허용하는 최적화 전략인 [Adaptive Behavior Policy Sharing](https://arxiv.org/abs/2002.05229)을 제안했다. 

또한 DeepMind는 에이전트의 최근 경험에 대해 K-최근접 이웃(K-Nearest Neighbours)을 사용하여 복잡한 탐색 게임을 해결하기 위해 지시된 탐색 정책(Directed Exploratory Policies)을 훈련시키는 [Never Give Up strategy](https://arxiv.org/abs/2002.06038)에 관한 연구를 2020년에 발표했다.



## Concluding Remarks

강화학습은 이론과 개념이 구체화되었떤 1950년대 이후, 전자 비디오 게임과 복잡한 보드게임인 체스, 바둑의 정복으로 이어지는, 신경망을 통한 이론의 응용으로 급속히 발전했다. 

특히 게임 분야에서의 강화학습의 활용은 연구자들에게 강화학습의 적용가능성과 한계에 대한 통찰을 주었다. 심층강화학습은 문제상황에 적합한 최고의 성능을 달성하기 위해 매우 높은 계산복잡도를 요구할 수 있다.

따라서 새로운 접근 방식이 연구되고 있으며 예를 들면, 다중환경교육(Multi-Environment Training) 및 언어모델링을 활용하여 상위수준의 의미를 추출하는 방식으로 보다 효율적으로 학습하는 연구 등이 있다.

그러나 강화학습이 일반적으로 게임과 같은 제한된 환경에서 가장 잘 작동한다는 점을 고려하면, 강화학습이 일반인공지능을 향한 길인지에 대한 질문은 여전히 열려있다. 일반적으로 강화학습이 활용되기 어려운 가장 큰 이유는, 적합한 환경설정 및 에이전트와의 상호작용을 제대로 정의하기 어렵기 때문인 것 같다. 

예를 들면, 나는 과거에 (단순히 아이디어 수준이지만) 원자 스케일의 공간을 환경으로 설정하고, 그 안에서 특정한 함수값을 최소화하는 루트를 찾는 에이전트의 학습을 구현하려고 했던 적이 있다. 그러나 구현에는 실패했는데, 그 이유는 실제 물리를 반영하는 환경 및 상호작용을 정의하기가 매우 어려웠기 때문이다. 이런 면이 해결되지 않는다는 강화학습의 일반화 성능을 향상시키는 것은 매우 어렵다.

이번 글에서는 강화학습의 기원과 발전을 아주 살짝 들여다보았다. 기계학습과 인공지능의 더 포괄적인 역사에 관심이 있다면 [기계학습의 역사](https://researchdatapod.com/the-history-of-machine-learning)를 읽어보자.

---




# An introduction to Q-Learning : Reinforcement Learning

이제 강화학습의 기본 개념에 대해 배우고 Q-Learning이라는 간단한 RL 알고리즘을 구현해보자.

<img src = 'https://blog.floydhub.com/content/images/size/w2000/2019/05/ricardo-gomez-angel-433649-unsplash.jpg'>

강아지와 같은 애완동물을 훈련시킬 때, 어떤 명령을 강아지에게 요청하고 강아지가 잘 따르면 먹이를 보상으로 주는 것을 본 적이 있을 것이다. 동물이 학습하는 이러한 방식은 결국 특정한 행동에 대한 보상을 주는 것과 연관이 있다.

그리고 이와 유사한 방식을 로봇이나 소프트웨어 안에 모델링하여 유용한 일을 할 수 있다는 것을 알아보자. 강화학습에 대한 소개 글이므로, 가장 간단한 강화학습 알고리즘인 Q-Learning의 예제에 대해 공부할 것이다.



## The brass tacks: What is reinforcement learning?

<br>
<p align="center" style="color:gray">
    <img src="https://blog.floydhub.com/content/images/2019/05/Rl_agent.png">
</p>
<center><b></b></center>
<br>

애완동물이 학습되는 방식과 매우 유사하게 인간 또한 학습을 한다. 인간은 살아가면서 여러가지 행동을 하는데, 그러한 행동 중 일부는 우리에게 좋은 보상을 가져다주고, 다른 어떤 것은 그렇지 않다.

강화학습의 관점에서 인간의 삶을 표현하자면 다음과 같다.

우리의 현실은 우리가 수많은 행동(Action)을 하는 환경(Environment)를 포함하고 있다. 우리는 목표를 달성하기 위해 특정한 행동을 하고, 그 일부에 대해 긍정적이거나 부정적인 보상을 받는다. 삶의 전체 과정에 걸쳐, 정신적/육체적 상태(State)는 지속적으로 변화하고, 그 과정에서 최대한 많은 보상(Reward)을 받을 수 있도록 행동을 강화한다.

강화학습에서 관심의 대상은 환경(Environment), 행동(Action), 보상(Reward), 상태(State)이다. 강화학습의 대상이 인간이라면, 그 인간은 삶의 전체게임에서 대리인(Agent)로 불린다. 우리는 삶을 탐색(Exploring)하고 행동, 보상 및 상태를 통해 학습하는 이 전체 패러다임은 강화학습의 토대를 만든다. 글을 읽으면서 느끼겠지만, 인간 역시 동물이므로 동물학습에 기원을 둔 강화학습의 설명과 매우 잘 맞아떨어지는 것 같다.

이러한 강화학습의 전체 아이디어는 본질적으로 매우 경험적(Empirical)이다. 결국 에이전트는 수많은 상태에서 수많은 행동을 하면서 최적화될 것이기 때문이다.

예를 들면 우리가 게임을 하거나 물건을 옮기는 것과 같은 일을 학습하려고 한다고 가정하자. 이러한 작업은 입력을 출력에 매핑하는 비선형 함수를 찾는 딥러닝의 지도학습이나, 입력 데이터에서 숨겨진 표현(Representation)을 찾는 것과는 완전히 다르다. 결국 컴퓨터가 이런 작업을 수행할 수 있으려면 기존과 다른 학습 패러다임이 필요하다. 

[Andriy Burkov](https://blog.floydhub.com/best-deep-learning-books-updated-for-2019/#hundred-page-machine-learning)는 그의 **Hundred Page Machine Learning Book**에서 강화학습을 다음과 같이 설명한다.

> Reinforcement learning solves a particular kind of problem where decision making is sequential, and the goal is long-term, such as game playing, robotics, resource management, or logistics.

즉, 강화학습은 순차적이고 장기적인 목표를 갖는 특정한 문제를 해결할 때 적합하다는 뜻이다.

만약 우리가 로봇을 강화학습으로 훈련한다고 하면, 환경은 로봇이 훈련되는 공간이다. 당연히 로봇 자체가 에이전트가 된다. 

만약 로봇이 창고에 있는데, 창고 안의 한 위치에서 다른 위치로 재료를 옮기는 것을 학습하려고 한다면, 이러한 작업은 환경을 항상 포함하고, 에이전트가 해당 환경에서 학습할 것을 기대하게 된다. 즉, 전통적인 머신러닝 알고리즘이 적용되기 어렵고, 강화학습이 필요한 상황이다.

게임세계를 뒤흔든 헤드라인 [AI Bots Join Forces To Beat Top Human Dota 2 Team](https://www.forbes.com/sites/samshead/2018/08/06/elon-musk-openai-bots-dota-2/?sh=68c6e3d2494c)은 강화학습의 직접적인 부산물이다. 게임 플레이에서 인간을 이기는 것을 넘어서, 강화학습의 다른 놀라운 사용 사례들은 이미 많이 있다.

* [Optimizing business processes](https://www.technologyreview.com/2016/03/18/161519/this-factory-robot-learns-a-new-job-overnight/)
* [Minimizing energy costs](https://www.deepmind.com/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-40)
* [Maximizing revenue shares of a company](https://arxiv.org/pdf/1803.09967.pdf)
* [More](https://www.oreilly.com/radar/practical-applications-of-reinforcement-learning-in-industry/)



## Recipes for reinforcement learning

이제부터 가장 간단한 강화학습인 Q-Learning 예제를 공부해보자. 

강화학습 문제를 다루는 상황에서는 문제에 대한 개요를 설정하는 것이 좋다. 즉, 강화학습의 주요 구성요소인 에이전트, 환경, 행동, 보상 및 상태를 정의해야 한다.



### Defining the problem statement

이제부터 창고 안에서 물건을 옮기기 위한 자율로봇을 만든다고 하자. 로봇은 제품의 제작에 필요한 여러 부품을 전달하여 작업하는 사람을 도울 것이다. 이러한 서로 다른 부품들은 창고 안의 9개의 서로 다른 위치에 놓여있다. 이 중에서 작업자는 특정한 물품이 놓여있는 위치를 가장 중요하다고 우선시한다. 창고 안의 위치가 다음과 같다고 하자.

<br>
<p align="center" style="color:gray">
    <img src="https://blog.floydhub.com/content/images/2019/05/image.png">
</p>
<center><b>A sample environment</b></center>
<br>

자세히 보면 각각의 위치 사이에 실선으로 표시된 장애물이 있다. 그림에서 **L6**은 작업자가 최우선으로 중요시하는 위치이다. 이제 우리는 로봇이 주어진 위치에서 다른 위치로의 최단경로를 스스로 찾을 수 있도록 하려고 한다.

이 경우 직관적으로 에이전트는 로봇이다. 그리고 환경은 창고가 될 것이다.



### The states

상태는 위치(Location)이다. 로봇이 특정시간에서 특정위치에 있다면 그것은 하나의 상태를 나타낸다. 그런데 기계는 문자보다는 숫자를 더 잘 이해하므로, 다음과 같이 위치코드를 정수로 맵핑하자.

<br>
<p align="center" style="color:gray">
    <img src="https://blog.floydhub.com/content/images/2019/05/image-1.png">
</p>
<center><b>Locations mapped as states</b></center>
<br>



### The actions

여기서 행동은 로봇이 특정 위치에서 이동할 수 있는 다른 직접적인 위치이다. 예를 들어, 로봇이 L8 위치에 있고 이동해서 도달할 수 있는 직접적인 위치가 L5, L7, L9라고 가정하자. 즉, 다음 그림과 같은 상황이다.

<br>
<p align="center" style="color:gray">
    <img src="https://blog.floydhub.com/content/images/2019/05/image-2.png">
</p>
<center><b>Sample actions</b></center>
<br>

직관적으로 생각하면, 여기서 에이전트가 취할 수 있는 행동(Action)의 집합은 결국 에이전트의 가능한 모든 상태(State)의 집합이다. 각각의 위치에 대해 로봇이 수행할 수 있는 행동의 집합은 매번 다를 것이다. 예를 들어, 로봇이 L1 위치에 있다면 행동의 집합도 변한다.



### The rewards

정리하면 우리는 다음과 같은 2가지 집합을 갖고 있다.

* 상태(State) 집합
    $S = {0, 1, 2, 3, 4, 5, 6, 7, 8}$
* 행동(Action) 집합
    $A = {0, 1, 2, 3, 4, 5, 6, 7, 8}$


이제부터 에이전트가 특정한 위치에서 다른 위치(상태)에 직접 도달할 수 있는 경우, 로봇에게 보상(Reward)을 줄 것이다. 

예를 들면, L9 위치는 L8 위치에서 직접 연결된다. 따라서 로봇이 L8에서 L9로, 또는 그 반대로 이동하면 1 만큼의 보상을 줄 것이다. 그러나 특정 위치에서 다른 위치에 직접 도달할 수 없다면, 보상은 0으로 제공해서 실질적으로 보상을 주지 않을 것이다.

이러한 보상 시스템을 통해 로봇은 자신의 움직임을 이해하여 직접 도달할 수 있는 위치와 도달할 수 없는 위치를 구별할 수 있다. 이러한 제한조건을 사용하여, 모든 가능한 상태(위치) 사이의 보상여부를 매핑하는 보상테이블(Reward Table)을 만들 수 있다.

<br>
<p align="center" style="color:gray">
    <img src="https://blog.floydhub.com/content/images/2019/09/table-of-rewards.png">
</p>
<center><b>Table of rewards</b></center>
<br>

위 그림의 보상 테이블에는, 로봇이 서로 다른 상태 사이를 이동할 때 얻을 수 있는 모든 가능한 보상에 대한 정보가 있다. 그런데 이 보상 테이블에 포함되지 않은 정보가 아직 존재한다.

위의 글을 다시 읽어보면, 작업자가 L6 위치에 최고의 우선순위를 부여했다고 말했다. 이것은 일종의 제한조건(단서)일 것이다. 그렇다면 이 사실을 보상 테이블에 어떻게 반영할까?

가장 우선순위가 높은 위치 L6에 일반적인 수준의 보상보다 훨씬 큰 값을 할당하는 방식으로 반영될 수 있다. 즉, (L6, L6)에 999를 할당하자.

<br>
<p align="center" style="color:gray">
    <img src="https://blog.floydhub.com/content/images/2019/12/reward-matrix-fixed.png">
</p>
<center><b>Table of rewards with a higher reward for the topmost location</b></center>
<br>

이제 강화학습을 다룰 때 필요한 모든 구성요소를 정의하였다. 이제부터 이를 기반으로 강화학습에서 널리 사용되는 몇가지 기본개념을 공부할 것이다. 가장 중요한 Bellman Equation부터 알아보자!



### The Bellman Equation

설명을 쉽게 하기 위해, 위에서 언급된 문제는 잠시 잊자.

그리고 위의 문제상황과 유사하지만, 이번에는 장벽 제한이 없는 다음 환경을 고려하자.

<br>
<p align="center" style="color:gray">
    <img src="https://blog.floydhub.com/content/images/2019/05/image-12.png">
</p>
<center><b>An empty environment</b></center>
<br>

다음 그림과 같이 로봇이 지정된 방향을 따라 현재위치(A)에서 녹색으로 표시된 위치로 이동해야 한다고 하자.

<br>
<p align="center" style="color:gray">
    <img src="https://blog.floydhub.com/content/images/2019/05/image-13.png">
</p>
<center><b>Sample environment, agent and directions to proceed</b></center>
<br>

로봇이 우리의 의도대로 움직이도록 프로그래밍하려면 어떻게 해야 할까? 한 가지 아이디어는 다음과 같이 로봇이 따라갈 수 있는 일종의 발자국(Footprint)를 도입하는 것이다.

<br>
<p align="center" style="color:gray">
    <img src="https://blog.floydhub.com/content/images/2019/05/image-14.png">
</p>
<center><b>An environment with value footprints</b></center>
<br>

로봇이 위에서 지정한 방향을 따를 경우 로봇의 경로를 따라오는 각 위치에 어떤 상수값을 지정한다. 로봇이 위치 A에서 시작하면, 이 상수값을 스캔할 수 있고 그에 따라 이동할 것이다.

하지만 이러한 발자국은 방향이 미리 고정되어 있고, 로봇이 항상 위치 A에서 시작하는 경우에만 유효하다. 이제 로봇이 다음 그림에서 표현된 위치에서 시작한다고 가정하자.

<br>
<p align="center" style="color:gray">
    <img src="https://blog.floydhub.com/content/images/2019/05/image-15.png">
</p>
<center><b>An environment with value footprint (contd.)</b></center>
<br>

이제 로봇은 위치 A에 있으므로 서로 다른 두 방향으로 이어지는 발자국을 볼 것이다. 따라서 목적지(녹색방)에 도달하기 위해 어느 방향으로 가야할지 결정할 수 없다. 이러한 상황은 로봇이 진행방향을 기억할 수단이 없기 때문에 발생한다.

따라서 이제 우리는 로봇에게 메모리(Memory) 기능을 만들어줄 것이다. 여기서 Bellman Equation이 등장한다.

$$ V(s)=\max _{a}\left(R(s, a) + \gamma V\left(s^{\prime}\right)\right) $$

기호의 의미는 다음과 같다.

* $s$ : 특정한 상태(State)
* $a$ : 방(위치) 사이를 이동하는 행동(Action) 
* $s′$ : 상태 s에서 로봇이 가는 다른 상태
* $\gamma$ : 할인계수(Discount Factor, 잠시 후에 설명)
* $R(s, a)$ : 상태 s와 행동 a를 입력받고 보상값을 출력하는 보상함수
* $V(s)$ : 특정한 상태 s에 있는 값 (the footprint)


즉, Bellman Equation에 의해 우리는 가능한 모든 행동을 고려하고, 최대값을 산출하는 행동을 취할 것이다.

그러나 footprint와 관련하여 한가지 제약조건(Constraint)이 있다. 즉, 녹색방 바로 아래에 노랑색으로 표시된 위치는 녹색방에 인접한 가장 가까운 방 중 하나임을 나타내기 위해 항상 1의 값을 갖는다는 것이다. 이것은 또한 로봇이 노랑색 위치에서 녹색방으로 이동할 때 보상을 받는 것을 확실히 하기 위함이다. 그리고 우리는 0.9의 Discount Factor를 가정할 것이다.

<br>
<p align="center" style="color:gray">
    <img src="https://blog.floydhub.com/content/images/2019/05/image-16.png">
</p>
<center><b>An environment with value footprint (contd.)</b></center>
<br>

위 그림과 같이 노랑색 위치의 바로 아래에서, $V(s)$는 얼마일까? 값을 방정식에 직접 대입하여 계산해보자.

$$ V(s)=\max _{a}(0 + 0.9 * 1)=0.9 $$

여기서 로봇은 노랑색으로 표시된 상태로 이동하는 것에 대한 보상을 받지 않을 것이다. 그러므로 일단 $R(s, a) = 0$이다. 로봇은 노랑색 위치에 있는 값을 알고 있으므로 $V(s') = 1$이다. 다른 위치에서도 이를 수행하면, 다음을 얻을 수 있다.

<br>
<p align="center" style="color:gray">
    <img src="https://blog.floydhub.com/content/images/2019/05/image-17.png">
</p>
<center><b>An environment with some value footprints computed from the Bellman equation</b></center>
<br>

여기서 주의해야 할 몇가지가 있다.

* **max()** 함수는 로봇이 항상 해당 상태에 있는 최대값을 제공하는 상태를 선택하도록 도와준다.
* Discount Factor **$\gamma$**는 로봇이 목적지(녹색방)에서 얼마나 멀리 떨어져 있는지를 알려준다. 이 값은 보통 알고리즘의 개발자가 지정한다.

동일한 방식으로 다른 모든 상태에 각각의 값을 지정할 수 있다.

<br>
<p align="center" style="color:gray">
    <img src="https://blog.floydhub.com/content/images/2019/05/image-18.png">
</p>
<center><b>An environment with all the value footprints computed from the Bellman equation</b></center>
<br>

위 그림과 같은 상황이 되면, 로봇은 환경 안의 어떤 위치에 놓여지더라도, 이러한 Value footprint를 활용하여 목적지(녹색방)으로 나아갈 수 있다. 

예를 들면, 로봇이 하늘색 위치에 도착하면 선택할 수 있는 상태는 2가지가 있다. Value Footprint가 배치되어 있기 때문에 두 경로 중 무엇을 선택해도 로봇이 취하기에 좋은 행동이 된다.

지금까지의 내용을 정리해보자. 특정한 위치에서 목적지에 도달하는 Main Task는 유사한 하위작업(Subtask)으로 세분화되었다. 즉, 반복적인 하위문제(Repetitive Subproblems)가 있는 문제를 해결하기 위해 특별하게 만들어진 프로그래밍 패러다임이 있는 것이다. 이를 [Dynamic Programming](http://smo.sogang.ac.kr/doc/bellman.pdf)이라 하는데, 우리가 방금까지 사용한 방정식을 만든 [Richard Bellman](https://en.wikipedia.org/wiki/Richard_E._Bellman)이 1954년에 발명했다. 따라서 Bellman Equation이라는 이름을 갖게 되었다. 이 방정식은 강화학습의 핵심 방정식 중 하나이다.

그러나 Bellman Equation을 고려하여 문제를 풀었음에도, 현실적으로 생각하면 우리의 주변 환경이 위의 알고리즘으로 잘 풀릴 것 같지는 않다.

왜냐하면 현실에는 항상 약간의 확률성(Stochasticity)이 관련되어 있기 때문이다. 이러한 확률성은 환경뿐만 아니라 로봇에도 적용된다. 예를 들면 로봇의 내부장치가 손상되거나, 로봇이 사전에 알지 못한 장애물에 부딛칠 수도 있을 것이다. 또는 로봇이 오른쪽 방향으로 회전하려고 했음에도 실제로 그 행동이 로봇의 의도와 다르게 일어나지 않을 수 있다. 

현실에서 우리의 모든 의도를 방해하는 이러한 확률성을 강화학습에 어떻게 도입할까? 마르코프 의사결정 과정(Markov Decision Process)이 있다.



### Modeling Stochasticity : Markov Decision Processes

다음 그림과 같이 로봇이 현재 빨강색 위치에 있고 녹색방으로 이동해야 한다고 하자.

<br>
<p align="center" style="color:gray">
    <img src="https://blog.floydhub.com/content/images/2019/05/image-21.png">
</p>
<center><b>An environment with an agent</b></center>
<br>

이제부터 우리는 로봇이 오작동(Dysfunctioning)할 가능성이 약간 있음을 고려할 것이다. 따라서 로봇이 현재의 빨강색 위치에서 목적지로 이동하기 위해 위쪽으로 움직이는 것이 아니라, 왼쪽이나 오른쪽 또는 아래쪽으로 움직일 수도 있다. 

이제 문제는 로봇이 위와 같은 환경에 있을 때, 이를 유연하게 처리하는 방법을 생각하는 것이다.

<br>
<p align="center" style="color:gray">
    <img src="https://blog.floydhub.com/content/images/2019/05/image-22.png">
</p>
<center><b>An environment with an agent (with stochasticity)</b></center>
<br>

이제 우리는 이전과 다르게 조금 더 복잡한 상황에 놓인다.

어떤 행동이 이루어져야 되는지에 대한 결정(Decision Making)이 부분적으로는 무작위(Partly Random)이고, 부분적으로는 로봇의 제어하에 있는(Partly Under the Control of the Robot) 상황이다.

즉, 로봇이 언제 오작동할지 알 수 없기 때문에 부분적으로 무작위이며, 로봇이 알고리즘에 의해 스스로 행동을 결정할 것이기 때문에 부분적으로는 로봇의 제어 하에 있다.

다음은 Wikipedia에 있는 Markov Decision Process의 정의이다. 

> A Markov decision process (MDP) is a discrete time stochastic control process. It provides a mathematical framework for modeling decision making in situations where outcomes are partly random and partly under the control of a decision maker.

즉, Markov Decision Process(MDP)는 이산시간 확률적 제어과정이라고 한다. 결과는 부분적으로 무작위이고, 부분적으로는 의사결정자의 통제 하에 있는 상황에서 의사결정을 모델링하기 위한 수학적 프레임워크를 제공한다. 우리는 정확히 이런 상황에 놓여있다.

그렇다면, "?부분적으로 무작위적이고 부분적으로 통제된 의사결정"이라는 개념을 어떻게 수학적으로 다루어야 할까? 놀랍게도 Bellman Equation에 약간의 조정만 하면 된다!

Bellman Equation의 원래 모양은 다음과 같았다.

$$ V(s)=\max _{a}\left(R(s, a) + \gamma V\left(s^{\prime}\right)\right) $$

여기에 임의성(Randomness)을 도입하려면 방정식에서 무엇을 바꿔야 할까?

로봇이 언제 오작동하여 의도된 행동과 다른 행동을 할지 확신할 수 없기 때문에, 현재 위치에서 목적지를 제외하면 어떤 위치 s'로 이동할 것인지 확신할 수 없다.

그러나 우리는 로봇이 취할 수 있는 모든 행동을 알고 있다! 따라서 각 행동에 대응하는 확률을 위의 방정식에 통합해야 한다. 즉, 특정한 행동을 할 확률이 $x$%임을 수치적으로 정하기 위해, 각 행동과 대응되는 확률을 연관시켜야 한다. 그렇게 하면 다음을 얻는다.

$$ V(s)=\max _{a}\left(R(s, a) + \gamma \sum{s^{\prime}} P\left(s, a, s^{\prime}\right) V\left(s^{\prime}\right)\right) $$

방정식에서 추가된 부분을 자세히 보자.

* $P(s, a, s')$ - 행동 a로 방 s에서 방 s'으로 이동할 확률
* $$\sum_{s^{\prime}} P\left(s, a, s^{\prime}\right) V\left(s^{\prime}\right)$$
    로봇이 임의성(Randomness)을 갖게 되는 상황에 대한 [기대값(Expectation)](https://en.wikipedia.org/wiki/Expected_value)

즉, 확률을 표현하는 항은 로봇이 현재위치(빨강색)에서 목적지(녹색방)으로 가기 위해 취할 수 있는 각각의 행동과 관련되어 있다고 가정하자. 결국 다음과 같은 상황을 만들 수 있을 것이다.

<br>
<p align="center" style="color:gray">
    <img src="https://blog.floydhub.com/content/images/2019/05/image-23.png">
</p>
<center><b>An environment with an agent (with probabilities)</b></center>
<br>

즉, 각각의 행동에 대응하는 확률을 연관시키면 로봇이 위쪽으로 이동할 확률이 80%임을 알 수 있다. 이제 모든 값을 방정식에 대입하면 다음을 얻는다.

$$ V(s)=\max _{a} (R(s, a) + \gamma((0.8V(room_{up})) +( 0.1V(room_{down})) + ...)) $$

이제 확률성(Stochasticity)이 도입된 상황이므로 기존의 Value Footprint가 변경된다. 그러나 지금 당장 변화된 Value Footprint를 계산하지는 않을 것이고, 대신 로봇이 스스로 알아내도록 할 것이다. (나중에 설명)

지금까지 우리는 로봇이 특정 위치로 이동하는 행동에 대해 보상을 하는 것을 고려하지 않았다. 현재 우리는 로봇이 목적지에 정확히 도착했을 때에만 보상을 할 것이다. 

이상적으로는 로봇이 스스로의 행동의 질적 평가를 할 수 있도록, 로봇이 취하는 모든 행동에 대한 보상이 있어야 한다. 보상이 항상 같을 필요는 없다. 그리고 보상이 전혀 없는 것보다는, 행동에 대한 어느 정도의 보상이 있는 편이 훨씬 좋다.

이러한 아이디어는 Living Penalty로 알려져 있다. 보상 시스템은 실제로 매우 복잡할 수 있으며, 특히 [희소 보상을 모델링하는 것](https://www.youtube.com/watch?v=0Ey02HT_1Ho)은 강화학습 도메인에서 활발한 연구 영역이다. 

이 주제에 대해 더 알고 싶다면 다음 자료를 참고하자.

* [자체지도예측(Self-supervised Prediction)을 통한 호기심-기반 탐색](https://pathak22.github.io/noreward-rl/)
* [강화학습의 호기심/미루기](https://ai.googleblog.com/2018/10/curiosity-and-procrastination-in.html)
* [희소하고 과소지정된 보상으로부터 일반화 학습](https://ai.googleblog.com/2019/02/learning-to-generalize-from-sparse-and.html)

이제 특정 위치로 이동하는 가치 $V(s)$를 보는 것보다 행동의 품질(Quality) 개념을 도입할 것이다.



## Transitioning to Q-Learning

지금까지 우리는 환경의 확률을 고려하여 특정 상태(위치)로 가는 값을 제공하는 방정식을 얻었다.

$$ V(s)=\max {a}\left(R(s, a)  + \gamma \sum{s^{\prime}} P\left(s, a, s^{\prime}\right) V\left(s^{\prime}\right)\right) $$

또한 로봇의 각 행동을 대응되는 보상과 연관시키는 것을 다루는 Living Penalty 개념에 대해 살짝 배웠다.

Q-Learning은 이동할 상태의 가능한 값(Value Footprint)을 결정하기보다는, 상태로 이동하기 위해 취하는 행동의 품질을 평가(Assessing the Quality of an Action)한다는 아이디어를 제시한다.

즉, 이전까지는 다음과 같은 상황이었다.

<br>
<p align="center" style="color:gray">
    <img src="https://blog.floydhub.com/content/images/2019/05/image-24.png">
</p>
<center><b>An environment with an agent (with possible value footprints)</b></center>
<br>

그러나 특정한 상태 $s'$으로 이동하기 위한 행동의 품질을 평가하는 아이디어를 통합하면 다음과 같이 된다.

<br>
<p align="center" style="color:gray">
    <img src="https://blog.floydhub.com/content/images/2019/05/image-25.png">
</p>
<center><b>An environment with an agent (with quality of actions)</b></center>
<br>

이제 로봇은 선택할 수 있는 4가지의 서로 다른 상태가 있고, 현재의 상태에 대해서도 취할 수 있는 4가지의 서로 다른 행동이 있다. 

그렇다면 로봇이 취할 수 있는 가능한 행동들의 누적품질(Cumulative Quality)인 $Q(s, a)$를 어떻게 계산할까? 하나씩 살펴보자.


이 방정식에서

$$ V(s)=\max_{a}\left(R(s, a) +  \gamma \sum_{s^{\prime}} P\left(s, a, s^{\prime}\right) V\left(s^{\prime}\right)\right) $$

**max()** 함수를 버리면, 다음을 얻는다.

$$ R(s, a) + \gamma \sum_{s^{\prime}}\left(P\left(s, a, s^{\prime}\right) V\left(s^{\prime}\right)\right) $$

본질적으로 $V(s)$를 계산하는 방정식에서, 우리는 가능한 모든 행동과 가능한 모든 상태를 (로봇의 현재 상태에서) 고려한 다음, 특정한 행동을 취함으로써 발생하는 최대값을 취할 것이다.

위의 방정식은 단 하나의 가능한 행동에 대한 Value Footprint를 생성한다. 우리는 이것을 행동의 품질(Quality of Action)으로 생각할 수 있다!

$$ Q(s, a)=R(s, a) + \gamma \sum_{s^{\prime}}\left(P\left(s, a, s^{\prime}\right) V\left(s^{\prime}\right)\right) $$

이제 특정한 행동의 품질을 수량화하는 방정식을 얻었으므로, 위의 방정식을 약간 조정할 것이다.

이제 $V(s)$는 $Q(s, a)$의 가능한 모든 값의 최대값(Maximum of all the possible values)이라 말할 수 있다. 이를 활용하여 $V(s')$을 $Q()$의 함수로 바꾸자.

$$ Q(s, a)=R(s, a) + \gamma \sum_{s^{\prime}}\left(P\left(s, a, s^{\prime}\right) \max _{a^{\prime}} Q\left(s^{\prime}, a^{\prime}\right)\right) $$

그런데 왜 이렇게 바꿀까? 답은 계산을 쉽게 하기 위해서이다.

왜냐하면 우리는 이제 계산하고자 하는 단 하나의 함수이면서 Dynamic Programming Paradigm의 핵심이기도 한 $Q()$를 갖고, $R(s, a)$는 특정 상태로 이동하는데 대한 보상을 생성하는 정량화된 메트릭(Quantified Metric)이기 때문이다.

여기서 행동의 품질은 Q-value로 불린다. 그리고 이제부터 Value Footprint를 Q-value로 부를 것이다.

이제 강화학습의 구현으로 넘어가기 전에 남아있는 마지막 한가지, Temporal Difference가 남아있다. 이에 대해 알아보자.



## The Last Piece of the Puzzle : Temporal Difference

위에서 Value Footprint를 계산하지 않고, 대신 로봇이 알아내도록 한다고 언급하고 넘어간 부분이 있었다.

Temporal Difference는 로봇이 시간에 따른 환경의 변화 각각에 대한 Q-Value를 계산하는데 도움이 되는 강화학습의 구성요소이다.

다음 그림에서 로봇이 현재 표시된 상태에 있고, 위의 상태로 이동하려 한다고 생각하자. 이때 로봇은 위로 이동하는 행동에 대한 Q-Value를 이미 알고 있다.

<br>
<p align="center" style="color:gray">
    <img src="https://blog.floydhub.com/content/images/2019/05/image-27.png">
</p>
<center><b>An environment with an agent</b></center>
<br>

환경은 본질적으로 확률적(Stochastic in Nature)이며, 위로 이동한 후 로봇이 얻을 보상은 이전의 관찰과 다를 수 있음을 알고 있다.

그렇다면 이러한 변화(Difference)를 어떻게 잡아낼까? 동일한 공식으로 새로운 $Q(s, a)$를 다시 계산하고, 이전에 알려졌던 $Q(s, a)$를 뺀다.

<br>
<p align="center" style="color:gray">
    <img src="https://blog.floydhub.com/content/images/2019/05/Screenshot-2019-05-20-at-18.02.14.png">
</p>
<center><b></b></center>
<br>

우리가 방금 도출한 방정식은 Q-Value의 Temporal Difference를 제공하는데, TD는 환경이 부여할 수 있는 임의의 변화를 포착하는데 도움이 된다. 새로운 $Q(s, a)$는 다음과 같이 업데이트된다.

$$ Q_{t}(s, a)=Q_{t-1}(s, a)+\alpha T D_{t}(a, s) $$

여기서 각 기호의 의미는 다음과 같다.

* $\alpha$ : 에이전트(로봇)이 환경에 의해 부여된 무작위적 변화에 얼마나 빠르게 적응하는지를 제어하는 학습률(Learning Rate)
* $ Q_{t} (s, a) $ : 현재의 Q-value
* $ Q_{t-1} (s, a) $ : 이전에 기록된 Q-value

따라서 시간적 차이를 의미하는 항 $TD_t(s, a)$를 바꾸면 다음과 같은 방정식을 얻는다.

$$ Q_{t}(s, a)=Q_{t-1}(s, a)+\alpha\left(R(s, a)+\gamma \max _{a^{\prime}} Q\left(s^{\prime}, a^{\prime}\right)-Q_{t-1}(s, a)\right) $$

이제 Q-Learning의 모든 기초적인 내용을 배웠으니 간단한 구현을 해보자! 처음에 설명했던 장벽제한이 있는 환경에서의 문제를 풀어보자!



## Implementing Q-Learning in Python with Numpy

Q-Learning을 구현하려면 먼저 창고의 위치를 다른 상태로 매핑할 수 있어야 한다. 앞서 설명한 샘플 환경을 기억하면서 시작해보자.

<br>
<p align="center" style="color:gray">
    <img src="https://blog.floydhub.com/content/images/2019/05/Screenshot-2019-05-20-at-18.03.38.png">
</p>
<center><b>A sample environment</b></center>
<br>

이제 창고 안의 각 위치(상태)를 정수에 매핑한다. 이렇게 하는 것은 계산상의 편의를 위해서이다.

``` Python
# Define the states
location_to_state = {'L1' : 0,
                     "L2' : 1,
                     "L3' : 2,
                     "L4' : 3,
                     "L5' : 4,
                     "L6' : 5,
                     "L7' : 6,
                     "L8' : 7,
                     "L9' : 8,
                     }
```

여기서는 Numpy 라이브러리만을 사용하도록 한다.

``` Python
import numpy as np
```

다음 단계는 위에서 언급했던 것처럼 "다음 상태로의 전환(Transition to the Next State)"을 나타내는 행동(Action)을 정의하는 것이다.

``` Python
# Define the actions
actions = [0, 1, 2, 3, 4, 5, 6, 7, 8]
```

이제 보상 테이블(Reward Table)을 정의하자.

``` Python
# Define the rewards
rewards = np.array([[0, 1, 0, 0, 0, 0, 0, 0, 0],
                    [1, 0, 1, 0, 1, 0, 0, 0, 0],
                    [0, 1, 0, 0, 0, 1, 0, 0, 0],
                    [0, 0, 0, 0, 0, 0, 1, 0, 0],
                    [0, 1, 0, 0, 0, 0, 0, 1, 0],
                    [0, 0, 1, 0, 0, 0, 0, 0, 0],
                    [0, 0, 0, 1, 0, 0, 0, 1, 0],
                    [0, 0, 0, 0, 1, 0, 1, 0, 1],
                    [0, 0, 0, 0, 0, 0, 0, 1, 0]])
```

위 배열을 보면 이미지에 표시된 것처럼 실제 장벽제한이 없는 것을 알 수 있다. 예를 들어, L4에서 L1으로의 전환은 허용되지만, 그 경로를 방해하기 위해 보상은 0으로 처리될 것이다.

위의 코드는 결국, 가능한 모든 상태들 사이의 직접 연결이 가능한지의 여부를 표현한 것이다. 그러나 아직 최우선순위를 갖는 위치(Top-Priority Location) L6는 고려하지 않았다.

<br>
<p align="center" style="color:gray">
    <img src="https://blog.floydhub.com/content/images/2019/09/table-of-rewards.png">
</p>
<center><b>Table of rewards (rewards matrix)</b></center>
<br>

또한, 특정 상태에서 다시 원래 상태로 돌아오는 것에 관한 정보인 역방향 매핑(Inverse Mapping From the States Back to original Location Indicators)이 필요하다. 

``` Python
# Maps indices to locations
state_to_location = dict((state,location) for location,state in location_to_state.items())
```

이제 Q-Learning 알고리즘의 두 매개변수인 $\alpha$(Learning Rate)와 $\gamma$(Discount Factor)를 명시하자.

``` Python
# Initialize parameters
gamma = 0.75 # Discount factor 
alpha = 0.9 # Learning rate 
```

이제 다음과 같은 ```get_optimal_route()``` 함수를 정의하는데, 이 함수는 다음을 할 것이다.

* 2가지 인수(Argument)를 받는다.
* 하나는 창고의 시작 위치(Starting Location in the Warehouse)
* 다른 하나는 최종 위치(End Location in the Warehouse)이다.
* 그리고 시작 위치에서 끝 위치에 도달하기 위한 최적의 경로를 문자가 포함된 정렬된 목록 형태로 반환한다.

처음에는 Q-Value를 모두 0으로 초기화한다.

``` Python
# Initializing Q-Values
Q = np.array(np.zeros([9,9]))
```

편의상, 보상행렬(Reward Matrix) rewards를 다른 변수에 복사하고 이에 대해 다룰 것이다.

``` Python
# Copy the rewards matrix to new Matrix
rewards_copy = np.copy(rewards)
```

```[9, 9]```로 총 9개의 위치가 있다. 함수는 하나의 시작 위치(Starting Location)와 하나의 끝 위치(Ending Location)를 인수로 가질 것이다. 

따라서 끝 위치의 우선순위를 999와 같은 더 큰 정수로 설정하고, 위치문자(예를 들면, L1, L2 등)로부터 끝 상태(Ending State)를 가져오자.

``` Python
# Get the ending state corresponding to the ending location as given
ending_state = location_to_state[end_location]

# With the above information automatically set the priority of the
# given ending state to the highest one
rewards_copy[ending_state,ending_state] = 999
```

여기서 학습(Learning)은 지속적인 프로세스이므로, 로봇이 잠시 동안 환경을 탐색(Explore)하도록 하고, 이를 단순히 1000번 반복수행할 것이다.

그런 다음 위에서 정의한 상태 집합에서 무작위로 하나의 상태를 선택하고 이를 ```current_state```라고 할 것이다.

``` Python
for i in range(1000):
    # Pick up a state randomly
    current_state = np.random.randint(0,9)
```

이제 루프 내부에서, 보상행렬(Reward Matrix)을 반복하여 무작위로 선택된 현재 상태에서 직접 도달할 수 있는 상태들을 가져오고, 해당 상태를 ```playable_actions``` 리스트에 할당한다.

아직 시작위치(Starting Location)에 대해서는 고려하지 않았다.

``` Python
playable_actions = []
# Iterate through the new rewards matrix and get the actions > 0
for j in range(9):
    if rewards_copy[current_state,j] > 0:
        playable_actions.append(j)
```

이제 ```playable_actions```에서 무작위로 하나의 상태를 선택한다.

``` Python
# Pick an action randomly from the list of playable actions leading us to the next 
# state
    next_state = np.random.choice(playable_actions)
```

Temporal Difference를 계산하고, 그에 따라 Q-value을 업데이트할 수 있다. 다음은 편의를 위한 Temporal Difference 공식이다.

<br>
<p align="center" style="color:gray">
    <img src="https://blog.floydhub.com/content/images/2020/02/td-formula-1.png">
</p>
<center><b></b></center>
<br>

다음은 Temporal Difference를 사용하여 Q-value을 업데이트하는 방법이다.

$$ Q_{t}(s, a)=Q_{t-1}(s, a)+\alpha T D_{t}(a, s) $$

<br>

``` Python
# Compute the temporal difference
# The action here exactly refers to going to the next state
TD = rewards_copy[current_state,next_state] + gamma * Q[next_state, np.argmax(Q[next_state,])] - Q[current_state,next_state]

# Update the Q-Value using the Bellman equation
Q[current_state,next_state] += alpha * TD
```

이제 전체 프로세스 중 가장 중요한 부분을 구현하였다. 지금까지 작성된 ```get_optimal_route()```는 다음과 같아야 한다.

``` Python
def get_optimal_route(start_location,end_location):
    # Copy the rewards matrix to new Matrix
    rewards_new = np.copy(rewards)
    
    # Get the ending state corresponding to the ending location as given
    ending_state = location_to_state[end_location]
    
    # With the above information automatically set the priority of the given ending 
    # state to the highest one
    rewards_new[ending_state,ending_state] = 999

    # -----------Q-Learning algorithm-----------
   
    # Initializing Q-Values
    Q = np.array(np.zeros([9,9]))

    # Q-Learning process
    for i in range(1000):
        # Pick up a state randomly
        current_state = np.random.randint(0,9) # Python excludes the upper bound
        # For traversing through the neighbor locations in the maze
        playable_actions = []
        # Iterate through the new rewards matrix and get the actions > 0
        for j in range(9):
            if rewards_new[current_state,j] > 0:
                playable_actions.append(j)
        # Pick an action randomly from the list of playable actions  
        # leading us to the next state
        next_state = np.random.choice(playable_actions)
        # Compute the temporal difference
        # The action here exactly refers to going to the next state
        TD = rewards_new[current_state,next_state] + gamma * Q[next_state,                                np.argmax(Q[next_state,])] - Q[current_state,next_state]
        # Update the Q-Value using the Bellman equation
        Q[current_state,next_state] += alpha * TD
```

이제 최적의 경로(Optimal Route)를 찾는 나머지 절반을 시작한다. 먼저 시작위치로 최적의 경로를 초기화하자.

``` Python
# Initialize the optimal route with the starting location
    route = [start_location]
```

우리는 현재 로봇의 다음 움직임(Next Move)에 대해 알지 못한다. 따라서 다음 위치도 시작위치로 설정한다.

<center>``` next_location = start_location ```</center><br>

최적경로를 찾기 위해 로봇이 수행할 정확한 반복 횟수를 모르기 때문에, 다음 위치가 끝 위치와 같지 않을 때까지 다음 프로세스를 반복한다. 

``` Python
# We don't know about the exact number of iterations needed to reach to the final 
# location hence while loop will be a good choice for iteratiing
    while(next_location != end_location):
        # Fetch the starting state
        starting_state = location_to_state[start_location]
        # Fetch the highest Q-value pertaining to starting state
        next_state = np.argmax(Q[starting_state,])
        # We got the index of the next state. But we need the corresponding letter. 
        next_location = state_to_location[next_state]
        route.append(next_location)
        # Update the starting location for the next iteration
        start_location = next_location
    
    return route
```

마지막으로 경로를 반환한다.

<center>``` return route ```</center>

전체 ```get_optimal_route()``` 함수는 다음과 같다.


``` Python
def get_optimal_route(start_location,end_location):
    # Copy the rewards matrix to new Matrix
    rewards_new = np.copy(rewards)
    
    # Get the ending state corresponding to the ending location as given
    ending_state = location_to_state[end_location]
    
    # With the above information automatically set the priority of  
    # the given ending state to the highest one
    rewards_new[ending_state,ending_state] = 999

    # -----------Q-Learning algorithm-----------
   
    # Initializing Q-Values
    Q = np.array(np.zeros([9,9]))

    # Q-Learning process
    for i in range(1000):
        # Pick up a state randomly
        current_state = np.random.randint(0,9) # Python excludes the upper bound
        
        # For traversing through the neighbor locations in the maze
        playable_actions = []
        
        # Iterate through the new rewards matrix and get the actions > 0
        for j in range(9):
            if rewards_new[current_state,j] > 0:
                playable_actions.append(j)
        
        # Pick an action randomly from the list of playable actions  
        # leading us to the next state
        next_state = np.random.choice(playable_actions)
        
        # Compute the temporal difference
        # The action here exactly refers to going to the next state
        TD = rewards_new[current_state,next_state] + gamma * Q[next_state,                         np.argmax(Q[next_state,])] - Q[current_state,next_state]
        
        # Update the Q-Value using the Bellman equation
        Q[current_state,next_state] += alpha * TD

    # Initialize the optimal route with the starting location
    route = [start_location]
    # We do not know about the next location yet, so initialize with the value of 
    # starting location
    next_location = start_location
    
    # We don't know about the exact number of iterations
    # needed to reach to the final location hence while loop will be a good choice 
    # for iteratiing
    
    while(next_location != end_location):
        # Fetch the starting state
        starting_state = location_to_state[start_location]
        
        # Fetch the highest Q-value pertaining to starting state
        next_state = np.argmax(Q[starting_state,])
        
        # We got the index of the next state. But we need the corresponding letter. 
        next_location = state_to_location[next_state]
        route.append(next_location)
        
        # Update the starting location for the next iteration
        start_location = next_location
    
    return route
```

`print(get_optimal_route('L9', 'L1'))`을 호출하면 다음이 얻어져야 한다.

`['L9', 'L8', 'L5', 'L2', 'L1']`

여기서 강화학습 알고리즘이 환경에 존재하는 장벽을 고려하는 방법에 유의하자. 예를 들면, $\alpha$ 및 $\gamma$ 매개변수를 변경하여 학습 프로세스가 어떻게 변경되는지 확인할 수 있다. 

이대로 강화학습을 마칠 수도 있지만, 약간 코드의 수준을 높여보자. 강화학습의 핵심은 에이전트와 환경의 상호작용이므로, 프로그래밍 관점에서 객체지향 프로그래밍(OOP) 방식으로 리팩토링되는 편이 더 자연스럽다.

`__init__` 을 제외하고, 다음 2가지 메서드를 포함하는 `QAgent()`라는 클래스를 정의하자.

* `training(self, start_location, end_location, iterations)` : 로봇이 환경에서 Q-Value를 얻는데 도움이 된다.
* `get_optimal_route(self, start_location, end_location, next_location, route, Q)` : 로봇이 한 위치에서 다른 위치까지의 최적의 경로를 가져온다.

먼저 클래스 생성자를 초기화할 `__init__()` 메서드를 정의한다.

``` Python
def __init__(self, alpha, gamma, location_to_state, actions, rewards, state_to_location, Q):
        
    self.gamma = gamma  
    self.alpha = alpha 

    self.location_to_state = location_to_state
    self.actions = actions
    self.rewards = rewards
    self.state_to_location = state_to_location

    self.Q = Q
```

이제 `training()` 메서드를 작성하자.

``` Python
def training(self, start_location, end_location, iterations):

    rewards_new = np.copy(self.rewards)

    ending_state = self.location_to_state[end_location]
    rewards_new[ending_state, ending_state] = 999

    for i in range(iterations):
        current_state = np.random.randint(0,9) 
        playable_actions = []

        for j in range(9):
            if rewards_new[current_state,j] > 0:
                playable_actions.append(j)

        next_state = np.random.choice(playable_actions)
        TD = rewards_new[current_state,next_state] + 
                self.gamma * self.Q[next_state, np.argmax(self.Q[next_state,])] -                                 self.Q[current_state,next_state]

        self.Q[current_state,next_state] += self.alpha * TD

    route = [start_location]
    next_location = start_location

    # Get the route 
    self.get_optimal_route(start_location, end_location, next_location, route, self.Q)
```

마지막은 `get_optimal_route()` 메서드이다.

``` Python
def get_optimal_route(self, start_location, end_location, next_location, route, Q):

    while(next_location != end_location):
        starting_state = self.location_to_state[start_location]
        next_state = np.argmax(Q[starting_state,])
        next_location = self.state_to_location[next_state]
        route.append(next_location)
        start_location = next_location

    print(route)
```

최종적으로 전체 클래스 정의는 다음과 같아야 한다.

``` Python
class QAgent():
    
    # Initialize alpha, gamma, states, actions, rewards, and Q-values
    def __init__(self, alpha, gamma, location_to_state, actions, rewards, state_to_location, Q):
        
        self.gamma = gamma  
        self.alpha = alpha 
        
        self.location_to_state = location_to_state
        self.actions = actions
        self.rewards = rewards
        self.state_to_location = state_to_location
        
        self.Q = Q
        
    # Training the robot in the environment
    def training(self, start_location, end_location, iterations):
        
        rewards_new = np.copy(self.rewards)
        
        ending_state = self.location_to_state[end_location]
        rewards_new[ending_state, ending_state] = 999
        
        for i in range(iterations):
            current_state = np.random.randint(0,9) 
            playable_actions = []

            for j in range(9):
                if rewards_new[current_state,j] > 0:
                    playable_actions.append(j)
    
            next_state = np.random.choice(playable_actions)
            TD = rewards_new[current_state,next_state] + \
                    self.gamma * self.Q[next_state, np.argmax(self.Q[next_state,])] - self.Q[current_state,next_state]
            
            self.Q[current_state,next_state] += self.alpha * TD

        route = [start_location]
        next_location = start_location
        
        # Get the route 
        self.get_optimal_route(start_location, end_location, next_location, route, self.Q)
        
    # Get the optimal route
    def get_optimal_route(self, start_location, end_location, next_location, route, Q):
        
        while(next_location != end_location):
            starting_state = self.location_to_state[start_location]
            next_state = np.argmax(Q[starting_state,])
            next_location = self.state_to_location[next_state]
            route.append(next_location)
            start_location = next_location
        
        print(route)
```

이제 다음과 같이 클래스 객체를 만들고 `training()` 메서드를 호출하자.

``` Python
qagent = QAgent(alpha, gamma, location_to_state, actions, rewards,  state_to_location, Q)
qagent.training('L9', 'L1', 1000)
```

이전 코드와 내용은 거의 동일하지만 리팩토링된 코드는 조금 더 우아하고 모듈식(Modular)으로 보인다. 출력도 당연히 동일하다.

``` ['L9', 'L8', 'L5', 'L2', 'L1'] ```


## Conclusion and further steps

이렇게 해서 강화학습 Newbie를 위한 1단계 포스팅이 마무리되었다.

참고로, 강화학습 관련 키워드를 검색하면 대부분 OpenAI에서 제공하는 gym이라는 라이브러리를 이용해서 CartPole과 같은 강화학습 예제를 푸는 방법을 보여주는데, 개인적으로 별로 만족스럽지 않았다.

왜냐하면, gym 관련 예제는 강화학습의 대표적인 구성요소인 상태, 행동, 환경 등의 요소에 대한 정의가 모두 gym으로 주어진 상태에서 활용만 하기 때문이다. 그러므로 그러한 라이브러리에 대한 의존성을 최소화하면서, 최대한 밑바닥에서 간단하더라도 원리 위주의 구현을 할 수 있는 자료로 공부하고 정리하게 되었다.

개인적으로 인공지능을 공부할 때는 이러한 밑바닥(Scratch)부터의 구현이 굉장히 도움이 된다고 생각한다. 



강화학습은 다양한 영역의 많은 문제에 대한 솔루션을 제공했다. 내가 특히 좋아하는 것은 주어진 데이터셋에 대한 최적의 신경망 아키텍처를 찾기 위해 심층강화학습(deep reinforcement learning)을 사용하는 [Google의 NasNet](https://arxiv.org/abs/1707.07012)이다.

이제 진지한 방식으로 강화학습을 시작하기 위한 몇가지 최고의 리소스를 검토해보겠다.

* **Richard S. Sutton** 과 **Andrew G. Barto**에 의해 쓰여진 [Reinforcement Learning, Second Edition: An Introduction](https://blog.floydhub.com/best-deep-learning-books-updated-for-2019/#reinforcement-learning-intro) : 강화학습의 교과서로 여겨진다.
* 조지아 대학교에서 설계하고 Udacity에서 제공하는 [Reinforcement Learning](https://www.udacity.com/course/reinforcement-learning--ud600)
* 메타학습과 강화학습의 결합에 관심이 있다면 이 [기사](https://blog.floydhub.com/meta-rl/)를 따를 수 있다.
* 딥러닝 + 강화학습의 결합 - [Deep RL Bootcamp](https://sites.google.com/view/deep-rl-bootcamp/lectures)
* [Deep Reinforcement Learning Hands-On](https://blog.floydhub.com/best-deep-learning-books-updated-for-2019/#deep-reinforcement-learning-hands-on) : Deep Q-Networks, Value Iteration, Policy Gradients 등과 같은 많은 최첨단 RL 개념을 다루는 Maxim Lapan의 책.
* Lex Fridman이 진행하는 [MIT Deep Learning](https://blog.floydhub.com/best-deep-learning-courses-updated-for-2019/#intro-to-rl) : 자율차량 시스템 등에서 다양한 딥러닝 어플리케이션이 어떻게 사용되는지 알려준다.
* 강화학습 게임의 주요 리더 중 한명이 가르치는 [Introduction to Reinforcement Learning - David Silver](https://blog.floydhub.com/best-deep-learning-courses-updated-for-2019/#adv-dl-rl) : 나는 2016년에 최초로 이 이름을 알게 되었다. 알파고의 창시자 중 한명이다.
* [Spinning Up in Deep RL](https://blog.floydhub.com/best-deep-learning-courses-updated-for-2019/#spinning-up) : 심층강화 학습에서 이론과 실습 사이의 점을 연결하는 가이드 역할의 자료. OpenAI 하우스에서 제공한다.
* [Controlling a 2D Robotic Arm with Deep Reinforcement Learning](https://blog.floydhub.com/robotic-arm-control-deep-reinforcement-learning/) : 자신만의 로봇 팔을 만드는 방법을 보여주는 기사
* [Spinning Up a Pong AI With Deep Reinforcement Learning](https://blog.floydhub.com/spinning-up-with-deep-reinforcement-learning/) : 클래식 비디오 게임 Pong을 단계별 방식으로 플레이하는 기본정책 그레디언트 모델을 소개하는 기사



## Reference

* https://researchdatapod.com/history-reinforcement-learning/
* https://blog.floydhub.com/an-introduction-to-q-learning-reinforcement-learning/

---
